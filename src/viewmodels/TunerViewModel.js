import { useEffect, useState, useRef, useCallback } from "react";
import { detectPitch } from "../utils/pitchDetection";
import { findClosestNote } from "../models/NoteModel";

export function useTunerViewModel() {
  const [noteData, setNoteData] = useState(null);
  const [rms, setRms] = useState(0);
  const [isRunning, setIsRunning] = useState(false);
  const [error, setError] = useState("");
  const [tunerMode, setTunerMode] = useState("auto"); // "auto" ho·∫∑c "manual"
  const [selectedString, setSelectedString] = useState("E2");

  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const bufferRef = useRef(null);
  const mediaStreamRef = useRef(null);
  const rafIdRef = useRef(null);
  const pitchHistory = useRef([]);
  const confidenceHistory = useRef([]);
  const lastPitchRef = useRef(null);
  const stableCountRef = useRef(0);
  const lastStablePitchRef = useRef(null);
  const lastUiUpdateAtRef = useRef(0);
  const currentNoteRef = useRef(null);

  // D·ª´ng micro + d·ªçn d·∫πp
  const stop = useCallback(() => {
    if (rafIdRef.current) cancelAnimationFrame(rafIdRef.current);
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach((t) => t.stop());
      mediaStreamRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }
    setIsRunning(false);
  }, []);

  // H√†m normalize pitch ‚Üí chia 2, chia 3 n·∫øu c·∫ßn
  function normalizePitch(freq) {
    let pitch = freq;
    // n·∫øu pitch cao h∆°n 500Hz ‚Üí c√≥ th·ªÉ l√† harmonic
    while (pitch > 500) pitch = pitch / 2;
    return pitch;
  }

  // H√†m l·∫•y t·∫ßn s·ªë target c·ªßa d√¢y ƒë√†n
  const getTargetFrequency = useCallback((note) => {
    const FREQUENCIES = {
      E2: 82.41,
      A2: 110.0,
      D3: 146.83,
      G3: 196.0,
      B3: 246.94,
      E4: 329.63,
    };
    return FREQUENCIES[note] || 0;
  }, []);

  const loop = useCallback(() => {
    if (!analyserRef.current || !bufferRef.current) return;
    analyserRef.current.getFloatTimeDomainData(bufferRef.current);

    // 1Ô∏è‚É£ RMS
    let r = 0;
    const buf = bufferRef.current;
    for (let i = 0; i < buf.length; i++) r += buf[i] * buf[i];
    r = Math.sqrt(r / buf.length);
    setRms(r);

    // 2Ô∏è‚É£ Ph√°t hi·ªán pitch v·ªõi confidence score
    const ctx = audioContextRef.current;
    const pitchResult = detectPitch(buf, ctx.sampleRate);

    if (pitchResult && pitchResult.freq) {
      let pitch = normalizePitch(pitchResult.freq);
      const confidence = pitchResult.confidence || 0.5; // Default confidence n·∫øu kh√¥ng c√≥
      const signalRms = pitchResult.rms || 0;

      console.log(
        `üéµ Raw pitch: ${pitchResult.freq.toFixed(2)} Hz ‚Üí Normalized: ${pitch.toFixed(2)} Hz | Confidence: ${(confidence * 100).toFixed(1)}% | RMS: ${signalRms.toFixed(4)}`
      );

      // 3Ô∏è‚É£ Enhanced smoothing v·ªõi confidence weighting
      const MAX_HISTORY = 8; // TƒÉng history ƒë·ªÉ smoothing t·ªët h∆°n
      const MIN_CONFIDENCE = 0.1; // Gi·∫£m ng∆∞·ª°ng confidence ƒë·ªÉ d·ªÖ detect h∆°n
      
      // Th√™m v√†o history (lu√¥n th√™m ƒë·ªÉ ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông)
      pitchHistory.current.push(pitch);
      confidenceHistory.current.push(confidence);
      
      if (pitchHistory.current.length > MAX_HISTORY) {
        pitchHistory.current.shift();
        confidenceHistory.current.shift();
      }

      // Weighted average d·ª±a tr√™n confidence
      if (pitchHistory.current.length >= 3) {
        let weightedSum = 0;
        let totalWeight = 0;
        
        for (let i = 0; i < pitchHistory.current.length; i++) {
          const weight = confidenceHistory.current[i] || 0.1;
          weightedSum += pitchHistory.current[i] * weight;
          totalWeight += weight;
        }
        
        const smoothed = weightedSum / totalWeight;

        // 4Ô∏è‚É£ Stability check - ch·ªâ c·∫≠p nh·∫≠t khi pitch ·ªïn ƒë·ªãnh
        const STABILITY_THRESHOLD = 2.0; // Hz - tƒÉng ƒë·ªÉ d·ªÖ ·ªïn ƒë·ªãnh h∆°n
        const MIN_STABLE_COUNT = 2; // Gi·∫£m xu·ªëng 2 l·∫ßn ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n
        
        if (!lastPitchRef.current || Math.abs(lastPitchRef.current - smoothed) > STABILITY_THRESHOLD) {
          stableCountRef.current = 0;
          lastPitchRef.current = smoothed;
        } else {
          stableCountRef.current++;
        }

        // Ch·ªâ c·∫≠p nh·∫≠t UI khi pitch ƒë√£ ·ªïn ƒë·ªãnh (b·ªè ƒëi·ªÅu ki·ªán confidence)
        if (stableCountRef.current >= MIN_STABLE_COUNT) {
          if (smoothed >= 70 && smoothed <= 500) {
            if (tunerMode === "auto") {
              // Ch·∫ø ƒë·ªô t·ª± ƒë·ªông: detect note g·∫ßn nh·∫•t
              const { note, targetFreq, cents } = findClosestNote(smoothed);

              // Hysteresis tr√°nh nh·∫£y note khi s√°t bi√™n
              const HYSTERESIS_CENTS = 20; // ~1/5 semitone
              const lastNote = currentNoteRef.current;
              let finalNote = note;
              let finalTarget = targetFreq;
              let finalCents = cents;
              if (lastNote && lastNote !== note && noteData?.note === lastNote && Math.abs(cents) < HYSTERESIS_CENTS) {
                finalNote = noteData.note;
                finalTarget = noteData.targetFreq;
                finalCents = 1200 * Math.log2(smoothed / finalTarget);
              }

              // Deadzone ƒë·ªÉ gi·∫£m rung
              const DEADZONE_CENTS = 6;
              if (Math.abs(finalCents) < DEADZONE_CENTS) finalCents = 0;

              console.log(
                `üéØ Auto Mode - Stable: ${smoothed.toFixed(2)} Hz | Note: ${finalNote} | Œî: ${finalCents.toFixed(1)} cents | Conf: ${(confidence * 100).toFixed(1)}%`
              );

              // Throttle c·∫≠p nh·∫≠t UI ~20fps
              const now = performance.now();
              if (now - lastUiUpdateAtRef.current > 50) {
                lastUiUpdateAtRef.current = now;
                setNoteData({ 
                  pitch: smoothed, 
                  note: finalNote, 
                  cents: finalCents, 
                  targetFreq: finalTarget, 
                  confidence,
                  isStable: true 
                });
                currentNoteRef.current = finalNote;
              }
            } else {
              // Ch·∫ø ƒë·ªô manual: so s√°nh v·ªõi d√¢y ƒë√£ ch·ªçn
              const targetFreq = getTargetFrequency(selectedString);
              let cents = 1200 * Math.log2(smoothed / targetFreq);

              // Gate: ch·ªâ c·∫≠p nh·∫≠t khi n·∫±m trong ¬±150 cents quanh d√¢y ch·ªçn
              const MANUAL_GATE_CENTS = 150;
              if (Math.abs(cents) > MANUAL_GATE_CENTS) {
                return;
              }

              // Deadzone ƒë·ªÉ gi·∫£m rung
              const DEADZONE_CENTS = 6;
              if (Math.abs(cents) < DEADZONE_CENTS) cents = 0;

              console.log(
                `üéØ Manual Mode - Stable: ${smoothed.toFixed(2)} Hz | Target: ${selectedString} (${targetFreq} Hz) | Œî: ${cents.toFixed(1)} cents | Conf: ${(confidence * 100).toFixed(1)}%`
              );

              const now = performance.now();
              if (now - lastUiUpdateAtRef.current > 50) {
                lastUiUpdateAtRef.current = now;
                setNoteData({ 
                  pitch: smoothed, 
                  note: selectedString, 
                  cents, 
                  targetFreq, 
                  confidence,
                  isStable: true 
                });
              }
            }
            lastStablePitchRef.current = smoothed;
          }
        }
      }
    } else {
      // Kh√¥ng c√≥ pitch detected - reset stability counter
      stableCountRef.current = 0;
    }

    rafIdRef.current = requestAnimationFrame(loop);
  }, [tunerMode, selectedString, getTargetFrequency]);

  const start = useCallback(async () => {
    try {
      setError("");
      stop();

      const AudioCtx = window.AudioContext || window.webkitAudioContext;
      const ctx = new AudioCtx();
      audioContextRef.current = ctx;

      if (ctx.state === "suspended") await ctx.resume();

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          channelCount: 1,
          sampleRate: 44100, // TƒÉng sample rate ƒë·ªÉ ƒë·ªô ch√≠nh x√°c cao h∆°n
          sampleSize: 16,
        },
      });

      mediaStreamRef.current = stream;
      const source = ctx.createMediaStreamSource(stream);
      const analyser = ctx.createAnalyser();
      analyser.smoothingTimeConstant = 0.1; // M·ªôt ch√∫t smoothing ƒë·ªÉ gi·∫£m nhi·ªÖu
      analyser.fftSize = 32768; // TƒÉng fftSize ƒë·ªÉ detect fundamental t·ªët h∆°n
      analyser.minDecibels = -90; // Gi·∫£m ng∆∞·ª°ng ƒë·ªÉ b·∫Øt √¢m thanh y·∫øu h∆°n
      analyser.maxDecibels = -10; // Gi·∫£m max ƒë·ªÉ tr√°nh clipping
      analyserRef.current = analyser;
      bufferRef.current = new Float32Array(analyser.fftSize);

      source.connect(analyser);
      setIsRunning(true);
      rafIdRef.current = requestAnimationFrame(loop);
    } catch (e) {
      console.error(e);
      setError(e?.message || "Kh√¥ng th·ªÉ truy c·∫≠p micro");
      stop();
    }
  }, [loop, stop]);

  useEffect(() => {
    return () => stop();
  }, [stop]);

  return { 
    noteData, 
    rms, 
    isRunning, 
    error, 
    start, 
    stop,
    tunerMode,
    setTunerMode,
    selectedString,
    setSelectedString
  };
}
